#Código del programa que se conecta al dataset dentro del sistema HDFS, para luego convertir en un dataframe de Spark
 
from pyspark.sql import SparkSession, functions as F

# Inicializa la sesión de Spark
spark = SparkSession.builder.appName('Tarea3').getOrCreate()

# Define la ruta del archivo .csv en HDFS
file_path = 'hdfs://localhost:9000/Tarea3/gt2j-8ykr.csv'

# Lee el archivo .csv, ajustando para 22 columnas y manteniendo el encabezado
df = spark.read.format('csv').option('header', 'true').option('inferSchema', 'true').load(file_path)

# Imprimir el esquema del DataFrame para revisar las columnas cargadas
df.printSchema()

# Mostrar las primeras filas del DataFrame
df.show(10)
# Eliminar filas donde haya valores nulos en columnas clave, por ejemplo, en 'id de caso' o 'fecha de reporte web'
df_clean = df.dropna(subset=['id de caso', 'fecha de muerte'])
df_clean.show(10)
# Eliminar filas duplicadas basadas en el 'id de caso'
df_clean = df_clean.dropDuplicates(['id de caso'])
df_clean.show(10)
# Crear una columna nueva que categorice las edades en rangos
df_transformed = df_clean.withColumn(
    "edad_categoria",
    F.when(F.col("edad") < 18, "Menor de edad")
     .when((F.col("edad") >= 18) & (F.col("edad") <= 60), "Adulto")
     .otherwise("Mayor de 60")
)
df_transformed.show(10)
# Extraer el año, mes y día de la columna 'fecha de muerte'
df_transformed = df_clean.withColumn("año_muerte", F.year(F.col("fecha_muerte"))) \
                         .withColumn("mes_muerte", F.month(F.col("fecha_muerte"))) \
                         .withColumn("día_muerte", F.dayofmonth(F.col("fecha_muerte")))

# Mostrar el DataFrame con las nuevas columnas de año, mes y día
df_transformed.select("fecha_muerte", "año_muerte", "mes_muerte", "día_muerte").show(10)
# Filtrar las filas donde 'fecha de muerte' no sea nula
df_fallecidos = df_transformed.filter(F.col("fecha_muerte").isNotNull()) 
 
 

#código para crear el  producer en Kafka 

import time
import json
import random
from kafka import KafkaProducer

def generate_sensor_data():
    return {
        "sensor_id": random.randint(1, 10),
        "temperature": round(random.uniform(20, 30), 2),
        "humidity": round(random.uniform(30, 70), 2),
        "timestamp": int(time.time())
    }

producer = KafkaProducer(bootstrap_servers=['localhost:9092'],
                         value_serializer=lambda x: json.dumps(x).encode('utf-8'))

while True:
    sensor_data = generate_sensor_data()
    producer.send('sensor_data', value=sensor_data)
    print(f"Sent: {sensor_data}")
    time.sleep(1) 
 

#código para crear el consumidor 

from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, window
from pyspark.sql.types import StructType, StructField, IntegerType, FloatType
import logging

# Configura el nivel de log a WARN para reducir los mensajes INFO
spark = SparkSession.builder \
    .appName("KafkaSparkStreaming") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

# Definir el esquema de los datos de entrada (sin TimestampType)
schema = StructType([
    StructField("sensor_id", IntegerType()),
    StructField("temperature", FloatType()),
    StructField("humidity", FloatType()),
    StructField("timestamp", IntegerType())  # Usamos Integer para luego convertirlo a timestamp
])

# Configurar el lector de streaming para leer desde Kafka
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "sensor_data") \
    .load()

# Parsear los datos JSON y convertir el timestamp de entero a formato de timestamp
parsed_df = df.select(from_json(col("value").cast("string"), schema).alias("data")).select("data.*") \
    .withColumn("timestamp", (col("timestamp").cast("timestamp")))  # Convertimos el entero a timestamp

# Calcular estadísticas por ventana de tiempo de 1 minuto
windowed_stats = parsed_df \
    .groupBy(window(col("timestamp"), "1 minute"), "sensor_id") \
    .agg({"temperature": "avg", "humidity": "avg"})

# Escribir los resultados en la consola
query = windowed_stats \
    .writeStream \ 
.outputMode("complete") \
    .format("console") \
    .start()

query.awaitTermination()
